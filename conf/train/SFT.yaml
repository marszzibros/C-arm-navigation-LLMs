output_dir: "SFT"      
num_train_epochs: 3
per_device_train_batch_size: 2
gradient_accumulation_steps: 32
gradient_checkpointing: true
learning_rate: 1e-4  
max_grad_norm: 0.3
warmup_ratio: 0.03
logging_steps: 1
optim: "adamw_8bit"
lr_scheduler_type: "cosine"
report_to: "wandb"
weight_decay: 0.01
remove_unused_columns: false
dataset_text_field: ""
save_strategy: "epoch"     
dataset_kwargs:
  skip_prepare_dataset: true
gradient_checkpointing_kwargs:
  use_reentrant: false
max_length: "none"

   